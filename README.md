<h1 align="center"> <p>ğŸ¤— LoRAì™€ SimCSEë¥¼ ì´ìš©í•œ<br>í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ Fine-Tuning</p></h1>
<h3 align="center">
    <p>Korean Embedding Model Fine-Tuning (with LoRA+SimCSE)</p>
</h3>

ìµœê·¼ ì–¸ì–´ëª¨ë¸ì˜ Fine-Tuningì—ì„œ ì ê·¹ì ìœ¼ë¡œ í™œìš©ë˜ëŠ” LoRAëŠ”, <br>
PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ 'Lora_config'í´ë˜ìŠ¤ë¥¼ í†µí•´ ì†ì‰½ê²Œ fine-tuning ì„¤ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.<br>
í•˜ì§€ë§Œ, ì„ë² ë”© ëª¨ë¸ì„ contrastive learningì˜ ëŒ€í‘œì ì¸ í•™ìŠµë°©ì‹ì¸ SimCSE ë§¤ì„œë“œë¥¼ í†µí•´ <br>ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê³  í•™ìŠµí•˜ëŠ” ë¶€ë¶„ì— ìˆì–´ì„œëŠ”
Lora_config í´ë˜ìŠ¤ì—ì„œ êµ¬ì²´ì ì¸ ì˜µì…˜ ì ìš©ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

ì´ì™€ ê°™ì€ ë¶ˆí¸í•¨ì„ í•´ì†Œí•˜ê³ ì, ì„ë² ë”© ëª¨ë¸ì„ SimCSE ë§¤ì„œë“œë¥¼ í†µí•´ í•™ìŠµí• ë•Œ,<br>
LoRA ë°©ì‹ì„ ì ìš©í•˜ì—¬ Fine-Tuning í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµ ì½”ë“œë¥¼ êµ¬í˜„í•˜ì˜€ìŠµë‹ˆë‹¤.

ê²½ë¡œë‚´ì— ì •í•´ì§„ í¬ë©§ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ êµ¬ì„±í•˜ê³  train.py íŒŒì¼ì— ì •ì˜ëœ argumentë¥¼ ì…ë ¥í•˜ì—¬ ì‹¤í–‰í•˜ë©´<br>
ê°„í¸í•˜ê²Œ LoRAë¥¼ ì ìš©í•œ SimCSE ë°©ì‹ì˜ Embedding Model Fine-Tuningì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.


## Quickstart

### Training Dataset setting<br>
- í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „, í•´ë‹¹ í”„ë¡œì íŠ¸ì˜ ê²½ë¡œì— ì•„ë˜ ë°ì´í„° í˜•ì‹ì„ ê°€ì§„ <br> normal_sentences.pkl ë˜ëŠ” strong_dataset.pkl íŒŒì¼ì´ ì…‹íŒ…ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.<br>

| ë°ì´í„° êµ¬ë¶„      | type                  | e.g.                                                                                                                                                                                      |
|------------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Normal Dataset** | List                  | ['ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?', 'ì§€ê¸ˆ ëª‡ ì‹œì•¼?', 'ê·¼ì²˜ ì¹´í˜ ì¶”ì²œí•´ ì¤˜.', 'ì˜¤ëŠ˜ ì¼ì • í™•ì¸í•´ ì¤˜.']                                                                                                                                 |
| **Strong Dataset** | a list of tuples      | 1) Pairs A: ['ì˜¤ëŠ˜ ì˜í™” ì¶”ì²œí•´ ì¤˜.', 'ê°€ì¥ ì¸ê¸° ìˆëŠ” ì¶•êµ¬ì„ ìˆ˜ê°€ ëˆ„êµ¬ì•¼?', 'ì˜ì–´ë¥¼ ì˜í•˜ëŠ” ë²•ì€?'] <br><br> 2) Pairs B: ['ì˜¤ëŠ˜ ê°™ì´ ë”ìš´ ì—¬ë¦„ì—ëŠ” íƒ€ì´íƒ€ë‹‰ì´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”', 'í˜¸ë‚ ë‘ì™€ ë©”ì‹œê°€ ì „ì„¸ê³„ì ìœ¼ë¡œ ë§ì€ ì¸ê¸°ë¥¼ ëˆ„ë¦¬ê³  ìˆì–´ìš”.', 'ìš°ì„  ì˜ì–´ ë‹¨ì–´ë¥¼ ì¶©ë¶„íˆ ìˆ™ì§€í•˜ëŠ”ê²Œ ìš°ì„ ì´ì—ìš”.'] |
- ### train.py ì‹¤í–‰ ì‹œ, 'data_type'ì„ '0' ìœ¼ë¡œ ì„¤ì •í•  ê²½ìš°?
  - normal datasetì„ ê¸°ì¤€ìœ¼ë¡œ ë™ì¼ë¬¸ì¥ì„ í•œìŒìœ¼ë¡œ ë¬¶ì€ê²ƒì„ positive pair,<br> ë‹¤ë¥¸ë¬¸ì¥ì„ í•œìŒìœ¼ë¡œ ë¬¶ì€ê²ƒì„ negative pairë¡œ ê°„ì£¼í•˜ì—¬ <br>SimCSE ë°©ì‹ì˜ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.
- ### train.py ì‹¤í–‰ ì‹œ, 'data_type'ì„ '1' ë¡œ ì„¤ì •í•  ê²½ìš°?
  - strong datasetì„ ê¸°ì¤€ìœ¼ë¡œ, ë‘ê°œì˜ listì˜ ë™ì¼ ì¸ë±ìŠ¤ì— ìœ„ì¹˜í•œ ë¬¸ì¥ì„ positive pairë¡œ ê°„ì£¼í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.


### Run training

```bash
python3 train.py --data_type 0 --fine-tune_model_yn 0
```

## Arguments

- ### --epochs 
  - Epoch íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
  - Default : 10
- ### --model_name
  - fine-tuningì„ ì§„í–‰í•  HuggingFaceì˜ backbone model(pre-trained model)<br>ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
  - Default : "BM-K/KoSimCSE-roberta-multitask"
- ### --fine_tune_model_yn
  - 0 : backbone modelì„ loadí•˜ì—¬ ìµœì´ˆ fine-tuningì„ ì§„í–‰í•  ê²½ìš°
  - 1 : LoRAë¥¼ í†µí•´ì„œ ì´ë¯¸ fine-tuningí•œ ëª¨ë¸ì„ loadí•´ì„œ ì¶”ê°€ fine-tuningí•  ê²½ìš°
  - Default : 0
- ### --lr
  - Learning_rate 
  - Default : 1e-6
- ### --data_type
  - 0 : normal datasetì„ í†µí•´ unsupervised learning ì§„í–‰
  - 1 : strong datasetì„ í†µí•´ supervised learning ì§„í–‰
- ### --test_set_shuffle
  - test datasetì„ shuffle ì—¬ë¶€ (0.ë¶€, 1:ì—¬)
  - Default : 0
- ### --loss_temperature
  - lossë¥¼ í†µí•œ backpropagation ì§„í–‰ ì‹œ, weight update ë°˜ì˜ ìˆ˜ì¤€ ì„¤ì •
  - Default : 0.1
- ### --batch_size
  - batch_size ì„¤ì •
  - Default : 64
- ### --train_mode
  - True : fine-tuning ì§„í–‰
  - False : Wandbë¥¼ í†µí•œ hyperparameter logging ì§„í–‰(model save í•˜ì§€ì•ŠìŒ)
  - Default : True